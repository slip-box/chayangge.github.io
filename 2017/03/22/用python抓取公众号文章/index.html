<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 用python抓取公众号所有文章 · 插秧哥's Blog</title><meta name="description" content="python爬虫 抓取公众号文章"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://chayangge.com/atom.xml" title="插秧哥's Blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/chayangge" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">用python抓取公众号所有文章</h1><div class="post-info">Mar 22, 2017</div><div class="post-content"><p>在我初学IT的时候，『网络爬虫』是个高大上的词汇，感觉『爬虫自动抓取』简直就是神一样的技术，不明觉厉，想到自己这么菜肯定学不会，后来了解<code>http</code>，猛然发现所谓网络爬虫本质上就是一个下载器而已。<br><a id="more"></a><br>为了消除爬虫的神秘感，重要的事情说三遍：爬虫就是一个下载器，一个下载器，下载器…</p>
<h2 id="那什么是下载器？"><a href="#那什么是下载器？" class="headerlink" title="那什么是下载器？"></a>那什么是下载器？</h2><p>下载器就是一个能发起http请求的东西，而浏览器可以理解为一个下载器和一个显示器的合体，那么网络爬虫就是一个能发起http请求，并从请求返回内容中筛选信息的程序。所以理解一个爬虫基本要一下几个硬知识：</p>
<ul>
<li>http是无状态的，谁都可以请求，且服务器不知道你是谁和上次什么区别（纯条件下，排除cookie、session技术）</li>
<li>凡是能发起请求，也就是可以网络编程的语言都可以写爬虫</li>
<li>各种语言为爬虫封装了各种请求框架、解析工具，写一个简单的爬虫真的很简单</li>
</ul>
<h2 id="用python写一个爬虫"><a href="#用python写一个爬虫" class="headerlink" title="用python写一个爬虫"></a>用python写一个爬虫</h2><p>python是门好语言，语法简洁优雅，还能做数据分析，爬虫技术更是相当成熟，各种包满足你各种要求，之前用python抓取过某网站的图片，感觉跟给力，现在用它做一个实用点的工具——抓取公众号文章。</p>
<p>公众号文章只能在手机上看，且查看历史还要一直往下滚，体验上和操作上都很麻烦，作为优质的公众号，想保存所有历史文章供日后慢慢欣赏在手机上很难实现，而有的PC网站上有但靠复制粘贴保存也费时费力，所以这个爬虫或许能解决这些问题。</p>
<p>首先要找到公众号文章资源，搜狗搜索提供微信公众号搜索，但只能前10篇，虽然后续可以一直抓取最新，但历史文章依然无法获取，还好有个网站，收录了公众号所有文章，就是这个<a href="http://chuansong.me/" target="_blank" rel="noopener">传送门</a>。用到了如下几个包：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line"><span class="keyword">import</span> requests <span class="comment">##发起请求</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment">##分析抓取结果</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure></p>
<h2 id="分析url"><a href="#分析url" class="headerlink" title="分析url"></a>分析url</h2><p>找到文章资源后就要分析页面了，我们以<a href="http://chuansong.me/account/caozsay" target="_blank" rel="noopener">caoz的梦呓</a>公众号为例，我们发现其地址信息为<code>http://chuansong.me/account/</code>+<code>公众号id</code>的形式，所以我们的爬虫要支持输入微信公众号id作为参数进行爬取:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys </span><br><span class="line"><span class="keyword">print</span> <span class="string">"正在搜索公众号："</span>, sys.argv[<span class="number">1</span>]</span><br><span class="line">wechat_id = sys.argv[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">"抓取地址："</span>, <span class="string">"http://chuansong.me/account/"</span> + wechat_id</span><br></pre></td></tr></table></figure></p>
<h2 id="分析页面"><a href="#分析页面" class="headerlink" title="分析页面"></a>分析页面</h2><p>分析页面源码发现DOM节点清晰明了，没有后续js动态渲染，简直是爬虫的最爱：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"question_link"</span> <span class="attr">href</span>=<span class="string">"/n/1669813351709"</span> <span class="attr">target</span>=<span class="string">"_blank"</span>&gt;</span></span><br><span class="line">赠人玫瑰，手有余香</span><br><span class="line"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">...</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">"question_link"</span> <span class="attr">href</span>=<span class="string">"/n/1663085851719"</span> <span class="attr">target</span>=<span class="string">"_blank"</span>&gt;</span></span><br><span class="line">Google关键词挖掘细分市场实战案例</span><br><span class="line"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>这里说明为什么要查看源码，因为有些网站为了反爬虫，把后台数据放在js里，让前端js进行后续渲染类似前端模板的作用，这样的html源码空空如也，非常不利于抓取，但还好我们有<code>PhantomJS</code>相当于一个无界面的浏览器，依然可以搞定，这里似乎可以明白：在互联网中，几乎没什么是抓不到的，差别只在抓取难度上而已，如果难度很大，成本高，则不如自己制造数据，这样抓取就不划算了。</p>
<p>如上源码所示，节点清晰明了，抓取非常容易，轻轻松松获取<code>href</code>即可。至于获取方法，<code>BeautifulSoup</code>提供了非常方便的方法，如果你用过jquery的话，那更是轻松自如了，这里查看源代码即可，不过多介绍。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">article_list = Soup.find_all(<span class="string">'a'</span>,class_=<span class="string">"question_link"</span>)</span><br><span class="line">page_list = Soup.select(<span class="string">".w4_5 &gt; span &gt; a"</span>)</span><br></pre></td></tr></table></figure></p>
<p>获得了节点序列，进行遍历，把遍历出的结果写入文件保存即可，无论是抓图还是抓取文字，都是这样。</p>
<h2 id="反爬虫"><a href="#反爬虫" class="headerlink" title="反爬虫"></a>反爬虫</h2><p>再次说到http，最开始http设计的时候，在当年的互联网环境下就是为了方便简洁，因此http被设计为无状态的，对服务器来说，不知道谁请求的和上次有什么关系，然而现在不能满足需求，为此设计了cookie、session技术来进行身份识别，这些是反爬虫的技术来源之一，而爬虫获取造成的服务器负担，在法律上目前属于灰色地带，如果一个网站每天被爬虫疯狂的访问，服务器看似很繁忙，然而却没有真实用户，也是件悲伤的事。更有甚者，犹豫爬虫的疯狂请求，导致服务器变慢，影响正常使用，这就算是攻击了。</p>
<p>所以发爬虫也很必要，常见策略有：数据用js动态加载，增加抓取难度，限制IP，如果某个IP访问频率很高，可封锁IP，但这要注意标准，以免误伤真实用户，然而封锁IP也不能从跟上解决，因为爬虫可以通过代理不停的更换IP，总之爬虫和反爬虫就是一个博弈，背后比的是成本，时间成本，技术成本，金钱成本。</p>
<h2 id="如何避免反爬虫"><a href="#如何避免反爬虫" class="headerlink" title="如何避免反爬虫"></a>如何避免反爬虫</h2><p>说白了就是伪装成真实用户，比如时间上不要这么疯狂，间隔个几秒，别被服务器反爬虫盯住，其次就是不停的更换请求头<code>User-Agent</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">user_agent_list = [</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+"</span>,</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure></p>
<p>这样服务器接收到请求信息就来自不同的浏览器，如果对于需要登录的网站，我们还要保存登录信息，维持cookie状态等，这里也有相应的框架和工具。</p>
<h2 id="爬虫优化"><a href="#爬虫优化" class="headerlink" title="爬虫优化"></a>爬虫优化</h2><p>一个能跑通的程序和可用之间还差很多细节，比如文件夹创建，保存目录，抓取提示，文件排序、代码优化等等，python并不是我的常用语言，整个实现过程也是边试边查文档，最后源代码放在<a href="https://github.com/chayangge/python-crawler" target="_blank" rel="noopener">这里</a></p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>下载源码后，安装相应的包，执行脚本+微信id 即可，然后就就可以看到终端显示的一条条住区进度了。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python crawler.py caozsay</span><br></pre></td></tr></table></figure></p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/03/28/python的GIL/" class="prev">上一篇</a><a href="/2017/03/16/node连上数据库/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'seansun';
var disqus_identifier = '2017/03/22/用python抓取公众号文章/';
var disqus_title = '用python抓取公众号所有文章';
var disqus_url = 'http://chayangge.com/2017/03/22/用python抓取公众号文章/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//seansun.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2021 <a href="http://chayangge.com">插秧哥</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>